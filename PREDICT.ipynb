{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Overview](#1)\n",
    "- [2. Function](#2)\n",
    "- [3. Training](#3)\n",
    "    - [3.1 Load Data Train](#31)\n",
    "    - [3.2 Train Model](#32)\n",
    "- [4. Predict Data Test](#4)\n",
    "    - [4.1 Create Data Test](#41)\n",
    "    - [4.2 Predict Probability](#42)\n",
    "    - [4.3 Predict Label with Probability Cut Off](#43)\n",
    "- [5. Save Results](#5)\n",
    "    - [5.1 Into Dataframe](#51)\n",
    "    - [5.2 Into csv file](#52)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "# 1. Overview\n",
    "Feature used in this model are inspired by [Grab](https://help.grab.com/driver/en-my/360001944868-Weekly-Safety-Report) and [Insurance Telematics paper by Peter Handel et. al](https://ieeexplore.ieee.org/document/6936433/authors#authors) as described below.\n",
    "\n",
    "| Peter Handel et. al Feature | Description                                                         | Weekly Safety Report Grab | Available Variable   |\n",
    "|---------------------|---------------------------------------------------------------------|--------------|----------------------|\n",
    "| Acceleration        | Number of rapid acceleration events and their harshness             | ✔            | Speed + Time, Gyro         |\n",
    "| Braking             | Number of harsh braking events and their harshness                  | ✔            | Speed + Time, Gyro         |\n",
    "| Speeding            | Amount of absolute speeding                                         | _              | Speed                |\n",
    "| Smoothness          | Long-term speed variations around a nominal speed                   | _              | Speed                |\n",
    "| Swerving            | Number of abrupt steering maneuvers and their harshness             |_             | Gyro + Acceleration  |\n",
    "| Cornering           | Number of events when turning at too high speed and their harshness | ✔            | Bearing + Gyro+ Time |\n",
    "| Elapsed time        | Time duration of the trip                                           |_              | Time                 |\n",
    "  \n",
    "Grab Telematics data consist of bookingID, Accuracy, Bearing, acceleration, gyro, second, and Speed. \n",
    "16135561 data point of telematics data transformed into 1 data point for each bookingID (transformed into total 20000 data point).\n",
    "\n",
    "\n",
    "| Original Feature              | Feature Aggregation per bookingID                                | Description                          |\n",
    "|----------------------|------------------------------------------------------------------|--------------------------------------|\n",
    "| second               | ```max_second```                                                       | `Elapsed Time`                                |\n",
    "| Speed                | ```mean_Speed```<br/>```median_Speed```<br/>```max_Speed```<br/>```std_Speed```<br/>```speed_diff``` | ```speed_diff``` is average of speed difference over time to estimate ```Smoothness``` along with `std_Speed`.<br/>```max_Speed```, `mean_Speed`, `median_Speed` estimate ```Speeding```.                      |\n",
    "| ```acceleration_(x,y,z)``` | ```mean_acceleration_(x,y,z)```<br/>```median_acceleration_(x,y,z)```<br/>```max_acceleration_(x,y,z)```<br/>```min_acceleration_(x,y,z)```<br/>```std_acceleration_(x,y,z)```<br/>```count(1,2,3)_acceleration_(x,y,z)```                                                  | ```count(1,2,3)_acceleration_(x,y,z)``` is how many times ```acceleration_(x,y,z)``` data goes to far from its median as described in advanced.pdf. Used to estimate harsh `Acceleration` and `Braking`.<br/><br/>Harsh Acceleration and Braking will result in medium(count2) to high(count3) acceleration_z value alteration                           |\n",
    "| gyro_(x,y,z)         | ```mean_gyro_(x,y,z)```<br/>```median_gyro_(x,y,z)```<br/>```max_gyro_(x,y,z)```<br/>```min_gyro_(x,y,z)```<br/>```std_gyro_(x,y,z)```<br/>```count(1,2,3)_gyro_(x,y,z)```                              | ```count(1,2,3)_gyro_(x,y,z)``` is how many times ```gyro_(x,y,z)``` data goes to far from its median as described in advanced.pdf. Used to estimate harsh `Acceleration`, `Braking`, and `Swerving`. <br/><br/>Harsh Acceleration and Braking create medium(count2) to high(count3) gyro value alteration, Swerving create high(count3) gyro value alteration      |\n",
    "\n",
    "\n",
    "The aggregated data become input to Stacking algorithm consist of LogisticRegression, RandomForestClassifier, and GradientBoostingClassifier that maximize AUC. Class prediction probability cut off is set to 0.5 by default, this nummber could be set to create a balance between TP and FP. \n",
    "\n",
    "</br>\n",
    "  \n",
    "_* Cornering detection isn't implemented due to insufficient time to integrate bearing + gyro + time_  \n",
    "_** This notebook contain only algorithm and final procedure of my solution to detect dangerous driving. More detailed explanation of exploratory data analysis, feature engineering, and thought process are in baseline(notebook|pdf) and advanced(notebook|pdf). **_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurniawanekn/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "#import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "#import swifter\n",
    "import sys\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, auc, classification_report \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contain function to save object & load object\n",
    "\n",
    "Used to save file to disk after aggregation to save time.\n",
    "Will not be used in submission.\n",
    "\"\"\"\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('/jet/prs/aiforsea/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('/jet/prs/aiforsea/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statform(df_dict,booking_id,var):\n",
    "    \"\"\"\n",
    "    Generate statistical measurement of a variable with the same bookingID.\n",
    "    Use this in pandas apply.\n",
    "    \n",
    "    Input:\n",
    "    booking_id      bookingID\n",
    "    df_dict         list containing dataframe per bookingID\n",
    "    data df         label data\n",
    "    \"\"\"\n",
    "    booking_id = booking_id.bookingID\n",
    "    std = df_dict[booking_id][var].std()\n",
    "    maxx = df_dict[booking_id][var].max()\n",
    "    minn = df_dict[booking_id][var].min()\n",
    "    mean = df_dict[booking_id][var].mean()\n",
    "    med = df_dict[booking_id][var].median()\n",
    "    \n",
    "    return std, maxx, minn, mean, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_statform(df_dict,data,booking_id,var):\n",
    "    \"\"\"\n",
    "    Generate statistical measurement of a variable with the same bookingID.\n",
    "    Use this in pandas apply.\n",
    "    \n",
    "    Input:\n",
    "    booking_id      bookingID\n",
    "    df_dict         list containing dataframe per bookingID\n",
    "    data df         label data\n",
    "    \"\"\"\n",
    "#     booking_id = booking_id.bookingID\n",
    "    var_series = df_dict[booking_id][var]\n",
    "    med = data.loc[data.bookingID == booking_id]['median_'+var].values[0]\n",
    "    mad = abs(var_series-med).mean()\n",
    "    \n",
    "    cnt = var_series.count()\n",
    "    \n",
    "    cut1 = 0\n",
    "    cut2 = 0\n",
    "    cut3 = 0\n",
    "## ver 4 gyro\n",
    "    if var=='gyro_x' or var=='gyro_y' or var=='gyro_z':\n",
    "        cut1 = len(np.where((abs(med-var_series)>(0.1)) & (abs(med-var_series)<=(0.2)))[0])\n",
    "        cut2 = len(np.where((abs(med-var_series)>(0.2)) & (abs(med-var_series)<=(0.4)))[0])\n",
    "        cut3 = len(np.where(abs(med-var_series)>(0.4))[0])\n",
    "    \n",
    "    if var=='acceleration_x' or var=='acceleration_y' or var=='acceleration_z':\n",
    "        cut1 = len(np.where((abs(med-var_series)>(1.0)) & (abs(med-var_series)<=(2.0)))[0])\n",
    "        cut2 = len(np.where((abs(med-var_series)>(2.0)) & (abs(med-var_series)<=(4.0)))[0])\n",
    "        cut3 = len(np.where(abs(med-var_series)>(4.0))[0])\n",
    "    \n",
    "#     return cut1,cut2,cut3,cut4,cut5\n",
    "    return mad,cut1,cut2,cut3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ediff(df_dict,booking_id,var):\n",
    "    #booking_id = booking_id.bookingID\n",
    "    var_series = df_dict[booking_id][var]\n",
    "    ediff = np.ediff1d(var_series)\n",
    "    return ediff.sum()/var_series.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "\n",
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"31\"></a>\n",
    "\n",
    "\n",
    "### 3.1 Load Data Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_obj('train_data')\n",
    "train_data_adv = load_obj('train_data_adv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"32\"></a>\n",
    "\n",
    "\n",
    "### 3.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(train_data, train_data_adv, on=['bookingID','label'])\n",
    "X = X.drop('label', axis=1)\n",
    "\n",
    "# drop column that didn't make any sense\n",
    "X.drop('bookingID', axis=1, inplace=True)\n",
    "X.drop(['median_second','mean_second','std_second','min_second'], axis=1, inplace=True) # no meaning, represented by max second\n",
    "X.drop(['min_Speed'], axis=1, inplace=True) # all 0 \n",
    "X.drop(['trip_duration'], axis=1, inplace=True) # max_second better\n",
    "\n",
    "Y = train_data_adv['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 61 at Level 0 \n",
      "4 models included in Level 0 \n",
      "Fold 1/4 , model 0 , auc===0.693837 \n",
      "Fold 1/4 , model 1 , auc===0.710841 \n",
      "Fold 1/4 , model 2 , auc===0.717507 \n",
      "Fold 1/4 , model 3 , auc===0.716961 \n",
      "=========== end of fold 1 in level 0 ===========\n",
      "Fold 2/4 , model 0 , auc===0.690396 \n",
      "Fold 2/4 , model 1 , auc===0.714664 \n",
      "Fold 2/4 , model 2 , auc===0.718058 \n",
      "Fold 2/4 , model 3 , auc===0.717392 \n",
      "=========== end of fold 2 in level 0 ===========\n",
      "Fold 3/4 , model 0 , auc===0.709514 \n",
      "Fold 3/4 , model 1 , auc===0.728336 \n",
      "Fold 3/4 , model 2 , auc===0.724586 \n",
      "Fold 3/4 , model 3 , auc===0.723911 \n",
      "=========== end of fold 3 in level 0 ===========\n",
      "Fold 4/4 , model 0 , auc===0.705526 \n",
      "Fold 4/4 , model 1 , auc===0.733591 \n",
      "Fold 4/4 , model 2 , auc===0.731622 \n",
      "Fold 4/4 , model 3 , auc===0.732037 \n",
      "=========== end of fold 4 in level 0 ===========\n",
      "Level 0, model 0 , auc===0.699819 \n",
      "Level 0, model 1 , auc===0.721858 \n",
      "Level 0, model 2 , auc===0.722943 \n",
      "Level 0, model 3 , auc===0.722575 \n",
      "Output dimensionality of level 0 is 4 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 179.824305 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 4 at Level 1 \n",
      "1 models included in Level 1 \n",
      "Fold 1/4 , model 0 , auc===0.721726 \n",
      "=========== end of fold 1 in level 1 ===========\n",
      "Fold 2/4 , model 0 , auc===0.722509 \n",
      "=========== end of fold 2 in level 1 ===========\n",
      "Fold 3/4 , model 0 , auc===0.733509 \n",
      "=========== end of fold 3 in level 1 ===========\n",
      "Fold 4/4 , model 0 , auc===0.740170 \n",
      "=========== end of fold 4 in level 1 ===========\n",
      "Level 1, model 0 , auc===0.729478 \n",
      "Output dimensionality of level 1 is 1 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 0.152308 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 179.983401 seconds \n"
     ]
    }
   ],
   "source": [
    "models=[\n",
    "        ######## First level ########\n",
    "        [RandomForestClassifier (n_estimators=100, max_depth=5, max_features='auto', random_state=1),\n",
    "        GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features='auto', random_state=1),\n",
    "        #DecisionTreeClassifier(),\n",
    "        LogisticRegression(random_state=1, solver='liblinear',class_weight='balanced'),\n",
    "        LogisticRegression(random_state=1, solver='liblinear'),\n",
    "        ],\n",
    "        ######## Second level ########\n",
    "        [LogisticRegression(random_state=1, solver='liblinear')]\n",
    "        ]\n",
    "\n",
    "model=StackNetClassifier(models, metric=\"auc\", folds=4,\n",
    "    restacking=False,use_retraining=True, use_proba=True, \n",
    "    random_state=12345,n_jobs=1, verbose=1)\n",
    "\n",
    "model.fit(X,Y)\n",
    "# model = RandomForestClassifier() # Backup just in case your pystacknet couldn't be installed properly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "\n",
    "## 4. Predict Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"41\"></a>\n",
    "\n",
    "\n",
    "### 4.1 Create Data Test Feature\n",
    "Please change according to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Change to TEST label file\n",
    "test_data = pd.read_csv('test/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv')\n",
    "\n",
    "# TEST feature file\n",
    "test_files = glob.glob(\"test/features/*.csv\")\n",
    "test_features = [pd.read_csv(f) for f in test_files]\n",
    "test_all_features = pd.concat(test_features,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# change dtype for a bit of efficientcy\n",
    "test_all_features = test_all_features.astype({\n",
    "    'bookingID': 'int64', \n",
    "    'Accuracy':'float32',\n",
    "    'Bearing':'float32',\n",
    "    'acceleration_x':'float32',\n",
    "    'acceleration_y':'float32',\n",
    "    'acceleration_z':'float32',\n",
    "    'gyro_x':'float32',\n",
    "    'gyro_y':'float32',\n",
    "    'gyro_z':'float32',\n",
    "    'second':'float32',\n",
    "    'Speed':'float32'\n",
    "})\n",
    "\n",
    "test_all_features.sort_values(['bookingID','second'], inplace=True) # sort here first for efficiency in the next process\n",
    "test_all_features.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_df_dict = {}\n",
    "\n",
    "for booking_id in test_data['bookingID']:\n",
    "\n",
    "    #df_dict[booking_id] = all_features.loc[all_features.bookingID==booking_id]\n",
    "    test_df_dict[booking_id] = copy.deepcopy(test_all_features.loc[test_all_features.bookingID==booking_id])\n",
    "    #df_dict[booking_id] = all_features.query('bookingID == @booking_id')\n",
    "    \n",
    "test_df_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# BASELINE FEATURE\n",
    "\n",
    "\n",
    "cols = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z', 'second', 'Speed'] \n",
    "\n",
    "for col in cols:\n",
    "    std_str = 'std_' + col\n",
    "    max_str = 'max_' + col\n",
    "    min_str = 'min_' + col\n",
    "    mean_str = 'mean_' + col\n",
    "    med_str = 'median_' + col\n",
    "    \n",
    "    test_data[[std_str, max_str, min_str, mean_str, med_str]] = test_data.apply(lambda x:statform(test_df_dict,x,col), axis = 1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# ADVANCED FEATURE\n",
    "\n",
    "\n",
    "# High & Low Pass Filter count\n",
    "\n",
    "\n",
    "test_data_adv = test_data[['bookingID','label']].copy()\n",
    "\n",
    "cols = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z'] \n",
    "\n",
    "for col in cols:\n",
    "    mad = 'mad_' + col\n",
    "    cut1 = 'count1_' + col\n",
    "    cut2 = 'count2_' + col\n",
    "    cut3 = 'count3_' + col\n",
    "    \n",
    "    test_data_adv[[mad,cut1, cut2, cut3]] = test_data_adv.apply(lambda x:adv_statform(test_df_dict,test_data,x['bookingID'],col), axis = 1, result_type='expand')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_adv['speed_diff'] = test_data_adv.apply(lambda x:ediff(test_df_dict,x['bookingID'],'Speed'), axis=1)\n",
    "test_data_adv['mounted'] = np.where((test_data['median_acceleration_y']>7) & (test_data['median_acceleration_y']<13), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"42\"></a>\n",
    "\n",
    "\n",
    "### 4.2 Predict Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.merge(test_data, test_data_adv, on=['bookingID'])[list_col]\n",
    "\n",
    "preds=model.predict_proba(X_test)\n",
    "preds_train=model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"43\"></a>\n",
    "\n",
    "\n",
    "### 4.3 Predict Label with Probability Cut Off "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "\n",
    "## 5. Save Results\n",
    "\n",
    "Change cut off threshold according to preferred risk appetite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "t = 0.5 # default 0.5\n",
    "t1 = 1 - t\n",
    "pred = pd.DataFrame({'pred0':preds[:,0],'pred1':preds[:,1]})\n",
    "pred['pred'] = np.where(pred['pred0']<=t1, '1', '0').astype(int)\n",
    "\n",
    "pred_train = pd.DataFrame({'pred0':preds_train[:,0],'pred1':preds_train[:,1]})\n",
    "pred_train['pred'] = np.where(pred_train['pred0']<=t1, '1', '0').astype(int)\n",
    "\n",
    "print (\"TRAIN\")\n",
    "print (\"AUC score : %.8f\" % roc_auc_score(Y, preds_train[:,1]))\n",
    "print (\"Confusion Matrix : \\n\", confusion_matrix(Y, pred_train.pred.values))\n",
    "print(classification_report(Y, pred_train.pred.values))\n",
    "print(\"\\nTrain Predict distribution\\n\",pd.Series(pred.pred.values).value_counts(normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nTest Predict distribution\\n\",pd.Series(pred.pred.values).value_counts(normalize=True)) # same distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"51\"></a>\n",
    "\n",
    "### 5.1 Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = pd.DataFrame({'bookindID':test_data.bookingID,'probability':preds[:,1]})\n",
    "\n",
    "\n",
    "pred_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"52\"></a>\n",
    "\n",
    "\n",
    "### 5.2 Into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
