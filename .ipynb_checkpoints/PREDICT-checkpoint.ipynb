{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Overview](#1)\n",
    "- [2. Function](#2)\n",
    "- [3. Training](#3)\n",
    "    - [3.1 Create Data Train (Optional)](#31)\n",
    "    - [3.2 Load Data Train](#32)\n",
    "    - [3.3 Train Model](#33)\n",
    "- [4. Predict Data Test](#4)\n",
    "    - [4.1 Create Data Test](#41)\n",
    "    - [4.2 Predict Probability](#42)\n",
    "    - [4.3 Predict Label with Probability Cut Off](#43)\n",
    "- [5. Save Results](#5)\n",
    "    - [5.1 Into Dataframe](#51)\n",
    "    - [5.2 Into csv file](#52)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "# 1. Overview\n",
    "Feature used in this model are inspired by [Grab](https://help.grab.com/driver/en-my/360001944868-Weekly-Safety-Report) and [Insurance Telematics paper by Peter Handel et. al](https://ieeexplore.ieee.org/document/6936433/authors#authors) as described below.\n",
    "\n",
    "| Peter Handel et. al Feature | Description                                                         | Weekly Safety Report Grab | Available Variable   |\n",
    "|---------------------|---------------------------------------------------------------------|--------------|----------------------|\n",
    "| Acceleration        | Number of rapid acceleration events and their harshness             | ✔            | Speed + Time, Gyro         |\n",
    "| Braking             | Number of harsh braking events and their harshness                  | ✔            | Speed + Time, Gyro         |\n",
    "| Speeding            | Amount of absolute speeding                                         | _              | Speed                |\n",
    "| Smoothness          | Long-term speed variations around a nominal speed                   | _              | Speed                |\n",
    "| Swerving            | Number of abrupt steering maneuvers and their harshness             |_             | Gyro + Acceleration  |\n",
    "| Cornering           | Number of events when turning at too high speed and their harshness | ✔            | Bearing + Gyro+ Time |\n",
    "| Elapsed time        | Time duration of the trip                                           |_              | Time                 |\n",
    "  \n",
    "Grab Telematics data consist of bookingID, Accuracy, Bearing, acceleration, gyro, second, and Speed. \n",
    "16135561 data point of telematics data transformed into 1 data point for each bookingID (transformed into total 20000 data point).\n",
    "\n",
    "\n",
    "| Original Feature              | Feature Aggregation per bookingID                                | Description                          |\n",
    "|----------------------|------------------------------------------------------------------|--------------------------------------|\n",
    "| second               | ```max_second```                                                       | `Elapsed Time`                                |\n",
    "| Speed                | ```mean_Speed```<br/>```median_Speed```<br/>```max_Speed```<br/>```std_Speed```<br/>```speed_diff``` | ```speed_diff``` is average of speed difference over time to estimate ```Smoothness``` along with `std_Speed`.<br/>```max_Speed```, `mean_Speed`, `median_Speed` estimate ```Speeding```.                      |\n",
    "| ```acceleration_(x,y,z)``` | ```mean_acceleration_(x,y,z)```<br/>```median_acceleration_(x,y,z)```<br/>```max_acceleration_(x,y,z)```<br/>```min_acceleration_(x,y,z)```<br/>```std_acceleration_(x,y,z)```<br/>```count(1,2,3)_acceleration_(x,y,z)```                                                  | ```count(1,2,3)_acceleration_(x,y,z)``` is how many times ```acceleration_(x,y,z)``` data goes to far from its median as described in advanced.pdf. Used to estimate harsh `Acceleration` and `Braking`.<br/><br/>Harsh Acceleration and Braking will result in medium(count2) to high(count3) acceleration_z value alteration                           |\n",
    "| gyro_(x,y,z)         | ```mean_gyro_(x,y,z)```<br/>```median_gyro_(x,y,z)```<br/>```max_gyro_(x,y,z)```<br/>```min_gyro_(x,y,z)```<br/>```std_gyro_(x,y,z)```<br/>```count(1,2,3)_gyro_(x,y,z)```                              | ```count(1,2,3)_gyro_(x,y,z)``` is how many times ```gyro_(x,y,z)``` data goes to far from its median as described in advanced.pdf. Used to estimate harsh `Acceleration`, `Braking`, and `Swerving`. <br/><br/>Harsh Acceleration and Braking create medium(count2) to high(count3) gyro value alteration, Swerving create high(count3) gyro value alteration      |\n",
    "\n",
    "\n",
    "The aggregated data become input to Stacking algorithm consist of LogisticRegression, RandomForestClassifier, and GradientBoostingClassifier that maximize AUC. Class prediction probability cut off is set to 0.5 by default, this nummber could be set to create a balance between TP and FP. \n",
    "\n",
    "</br>\n",
    "  \n",
    "_* Cornering detection isn't implemented due to insufficient time to integrate bearing + gyro + time_  \n",
    "_** This notebook contain only algorithm and final procedure of my solution to detect dangerous driving. More detailed explanation of exploratory data analysis, feature engineering, and thought process are in baseline(notebook|pdf) and advanced(notebook|pdf). **_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "import swifter\n",
    "import sys\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, auc, classification_report \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contain function to save object & load object\n",
    "\n",
    "Used to save file to disk after aggregation to save time.\n",
    "Will not be used in submission.\n",
    "\"\"\"\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('/jet/prs/aiforsea/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('/jet/prs/aiforsea/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statform(booking_id,var):\n",
    "    \"\"\"\n",
    "    Generate statistical measurement of a variable with the same bookingID.\n",
    "    Use this in pandas apply.\n",
    "    \n",
    "    Input:\n",
    "    booking_id      bookingID\n",
    "    df_dict         list containing dataframe per bookingID\n",
    "    data df         label data\n",
    "    \"\"\"\n",
    "    booking_id = booking_id.bookingID\n",
    "    std = df_dict[booking_id][var].std()\n",
    "    maxx = df_dict[booking_id][var].max()\n",
    "    minn = df_dict[booking_id][var].min()\n",
    "    mean = df_dict[booking_id][var].mean()\n",
    "    med = df_dict[booking_id][var].median()\n",
    "    \n",
    "    return std, maxx, minn, mean, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_statform(df_dict,data,booking_id,var):\n",
    "    \"\"\"\n",
    "    Generate statistical measurement of a variable with the same bookingID.\n",
    "    Use this in pandas apply.\n",
    "    \n",
    "    Input:\n",
    "    booking_id      bookingID\n",
    "    df_dict         list containing dataframe per bookingID\n",
    "    data df         label data\n",
    "    \"\"\"\n",
    "#     booking_id = booking_id.bookingID\n",
    "    var_series = df_dict[booking_id][var]\n",
    "    med = data.loc[data.bookingID == booking_id]['median_'+var].values[0]\n",
    "    mad = abs(var_series-med).mean()\n",
    "    \n",
    "    cnt = var_series.count()\n",
    "    \n",
    "    cut1 = 0\n",
    "    cut2 = 0\n",
    "    cut3 = 0\n",
    "## ver 4 gyro\n",
    "    if var=='gyro_x' or var=='gyro_y' or var=='gyro_z':\n",
    "        cut1 = len(np.where((abs(med-var_series)>(0.1)) & (abs(med-var_series)<=(0.2)))[0])\n",
    "        cut2 = len(np.where((abs(med-var_series)>(0.2)) & (abs(med-var_series)<=(0.4)))[0])\n",
    "        cut3 = len(np.where(abs(med-var_series)>(0.4))[0])\n",
    "    \n",
    "    if var=='acceleration_x' or var=='acceleration_y' or var=='acceleration_z':\n",
    "        cut1 = len(np.where((abs(med-var_series)>(1.0)) & (abs(med-var_series)<=(2.0)))[0])\n",
    "        cut2 = len(np.where((abs(med-var_series)>(2.0)) & (abs(med-var_series)<=(4.0)))[0])\n",
    "        cut3 = len(np.where(abs(med-var_series)>(4.0))[0])\n",
    "    \n",
    "#     return cut1,cut2,cut3,cut4,cut5\n",
    "    return mad,cut1,cut2,cut3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ediff(df_dict,booking_id,var):\n",
    "    #booking_id = booking_id.bookingID\n",
    "    var_series = df_dict[booking_id][var]\n",
    "    ediff = np.ediff1d(var_series)\n",
    "    return ediff.sum()/var_series.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "\n",
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"31\"></a>\n",
    "\n",
    "\n",
    "### 3.1 Create Data Train Feature (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"32\"></a>\n",
    "\n",
    "\n",
    "### 3.2 Load Data Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c2a108d7933f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data_adv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b93eb8c646cb>\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/jet/prs/aiforsea/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = load_obj('train_data')\n",
    "train_data_adv = load_obj('train_data_adv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "\n",
    "### 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(train_data, train_data_adv, on=['bookingID','label'])\n",
    "X = X.drop('label', axis=1)\n",
    "\n",
    "# drop column that didn't make any sense\n",
    "X.drop('bookingID', axis=1, inplace=True)\n",
    "X.drop(['median_second','mean_second','std_second','min_second'], axis=1, inplace=True) # no meaning, represented by max second\n",
    "X.drop(['min_Speed'], axis=1, inplace=True) # all 0 \n",
    "X.drop(['trip_duration'], axis=1, inplace=True) # max_second better\n",
    "\n",
    "Y = train_data_adv['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "modelx=StackNetClassifier(models, metric=\"auc\", folds=4,\n",
    "    restacking=False,use_retraining=True, use_proba=True, \n",
    "    random_state=12345,n_jobs=1, verbose=1)\n",
    "\n",
    "modelx.fit(X_train,y_train)\n",
    "# model = RandomForestClassifier() # Backup just in case your pystacknet couldn't be installed properly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "\n",
    "## 4. Predict Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"41\"></a>\n",
    "\n",
    "\n",
    "### 4.1 Create Data Test Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"42\"></a>\n",
    "\n",
    "\n",
    "### 4.2 Predict Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=modelx.predict_proba(X_test)\n",
    "preds_train=modelx.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"43\"></a>\n",
    "\n",
    "\n",
    "### 4.3 Predict Label with Probability Cut Off "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "\n",
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "t = 0.5 # default 0.5\n",
    "t1 = 1 - t\n",
    "pred = pd.DataFrame({'pred0':preds[:,0],'pred1':preds[:,1]})\n",
    "pred['pred'] = np.where(pred['pred0']<=t1, '1', '0').astype(int)\n",
    "\n",
    "pred_train = pd.DataFrame({'pred0':preds_train[:,0],'pred1':preds_train[:,1]})\n",
    "pred_train['pred'] = np.where(pred_train['pred0']<=t1, '1', '0').astype(int)\n",
    "\n",
    "print (\"TRAIN\")\n",
    "print (\"Accuracy  : %.8f\" % accuracy_score(y_train, pred_train.pred.values))\n",
    "print (\"Recall    : %.8f\" % recall_score(y_train, pred_train.pred.values))\n",
    "print (\"Precision : %.8f\" % precision_score(y_train, pred_train.pred.values))\n",
    "print (\"AUC score : %.8f\" % roc_auc_score(y_train, preds_train[:,1]))\n",
    "print (\"Confusion Matrix : \\n\", confusion_matrix(y_train, pred_train.pred.values))\n",
    "print(classification_report(y_train, pred_train.pred.values))\n",
    "print(\"Train predict distribution\\n\",pd.Series(pred.pred.values).value_counts(normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test predic distribution\",pd.Series(pred.pred.values).value_counts(normalize=True)) # same distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"51\"></a>\n",
    "\n",
    "### 5.1 Into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"52\"></a>\n",
    "\n",
    "\n",
    "### 5.2 Into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
